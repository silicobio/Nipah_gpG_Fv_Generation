{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e782d8",
   "metadata": {},
   "source": [
    "# Generate a novel antibody sequence with `peleke-mistral-7b-instruct-v0.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba740c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908c0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting single antigen antibody generation...\n",
      "Loading model: silicobio/peleke-mistral-7b-instruct-v0.2\n",
      "Loading Mistral model: silicobio/peleke-mistral-7b-instruct-v0.2...\n",
      "Detected PEFT model with base: mistralai/Mistral-7B-Instruct-v0.2\n",
      "Loading tokenizer from adapter path...\n",
      "Tokenizer loaded with vocab size: 32000\n",
      "Loading base model: mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model vocab size: 32000\n",
      "Loading PEFT adapters...\n",
      "\n",
      "============================================================\n",
      "VOCABULARY SIZE MISMATCH DETECTED\n",
      "============================================================\n",
      "The model has a vocabulary size mismatch.\n",
      "This typically happens when the model was fine-tuned with additional tokens.\n",
      "\n",
      "Attempting alternative loading method...\n",
      "Using manual vocabulary resize method...\n",
      "Loading base model: mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.56it/s]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force resizing embeddings to 32005\n",
      "Loading PEFT adapters with resized model...\n",
      "Setting up Mistral-specific tokenizer...\n",
      "Added 5 new tokens: ['<epi>', '</epi>', 'Antigen', 'Antibody', 'Epitope']\n",
      "Model loaded successfully with manual resize!\n",
      "Setting up Mistral-specific tokenizer...\n",
      "No new tokens needed\n",
      "Model loaded on device: cuda:0\n",
      "Final vocab size: 32005\n",
      "\n",
      "Generating 20 antibody samples for antigen...\n",
      "Antigen sequence: ICLQKTSNQILKPKLISYTLGQSGTCITDPLLAMDEGYFAYSHLERIG[S][C][S][R]GVSKQRIIGVGEVLDRGDEVPSLFMTNVWTPPNPNTVYHC...\n",
      "Converted format: ICLQKTSNQILKPKLISYTLGQSGTCITDPLLAMDEGYFAYSHLERIG<epi>S</epi><epi>C</epi><epi>S</epi><epi>R</epi>GVSK...\n",
      "Generating sample 1/20...\n",
      "Generating sample 2/20...\n",
      "Generating sample 3/20...\n",
      "Generating sample 4/20...\n",
      "Generating sample 5/20...\n",
      "  Generated 5/20 samples\n",
      "Generating sample 6/20...\n",
      "Generating sample 7/20...\n",
      "Generating sample 8/20...\n",
      "Generating sample 9/20...\n",
      "Generating sample 10/20...\n",
      "  Generated 10/20 samples\n",
      "Generating sample 11/20...\n",
      "Generating sample 12/20...\n",
      "Generating sample 13/20...\n",
      "Generating sample 14/20...\n",
      "Generating sample 15/20...\n",
      "  Generated 15/20 samples\n",
      "Generating sample 16/20...\n",
      "Generating sample 17/20...\n",
      "Generating sample 18/20...\n",
      "Generating sample 19/20...\n",
      "Generating sample 20/20...\n",
      "  Generated 20/20 samples\n",
      "Cleaning up model...\n",
      "\n",
      "Results saved to: single_antigen_antibodies.csv\n",
      "Total sequences generated: 20\n",
      "Successful generations: 20\n",
      "Failed generations: 0\n",
      "Average sequence length: 228.1 characters\n",
      "\n",
      "============================================================\n",
      "ANTIBODY SEQUENCE VALIDATION\n",
      "============================================================\n",
      "Valid antibody sequences: 0/20 (0.0%)\n",
      "\n",
      "Sample generated antibodies:\n",
      "\n",
      "Sample antibody_01:\n",
      "Length: 226 amino acids\n",
      "Sequence: VQLQESGPGLVKPSQSLSLTCTVTGYSITTGYAWNWIRQFPGNKLEWMGYISYSGSTYYPSLKSRISITRDTSKNQFFLQLSIVTTEDTATYYCARGTTL...\n",
      "\n",
      "Sample antibody_02:\n",
      "Length: 237 amino acids\n",
      "Sequence: QVQLQESGPGLVKPSETLSVTCTVSGGSIGSNNYWSWIRQPAGKGLEWIGYIYYSGSTNYNPSLKSRVTMSVDTSKNQFSLKLSSVTAADTAVYYCVRNY...\n",
      "\n",
      "Sample antibody_03:\n",
      "Length: 227 amino acids\n",
      "Sequence: QVQLVQSGAEVKKPGESLKISCKGSGYRFSSYWIGWVRQMPGKGLEWMGIIYPGDSDTRYSPSFQGQVTISADKSISTAYLQWSSLKASDTAMYYCARQG...\n",
      "\n",
      "Generation completed! Check 'single_antigen_antibodies.csv' for full results.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory and cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def setup_mistral_tokenizer(tokenizer, model):\n",
    "    \"\"\"Setup tokenizer with epitope and amino acid tokens for Mistral\"\"\"\n",
    "    print(\"Setting up Mistral-specific tokenizer...\")\n",
    "    \n",
    "    # Get existing vocabulary\n",
    "    existing_vocab = set(tokenizer.get_vocab().keys())\n",
    "    \n",
    "    # Define all tokens you want to add\n",
    "    epitope_tokens = [\"<epi>\", \"</epi>\"]\n",
    "    task_tokens = [\"Antigen\", \"Antibody\", \"Epitope\"]\n",
    "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    separator_tokens = [\"|\"]\n",
    "    all_desired_tokens = epitope_tokens + task_tokens + amino_acids + separator_tokens\n",
    "    \n",
    "    # Only add tokens that don't exist\n",
    "    tokens_to_add = [token for token in all_desired_tokens if token not in existing_vocab]\n",
    "    \n",
    "    if tokens_to_add:\n",
    "        num_added = tokenizer.add_special_tokens({\n",
    "            \"additional_special_tokens\": (tokenizer.additional_special_tokens or []) + tokens_to_add\n",
    "        })\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        print(f\"Added {num_added} new tokens: {tokens_to_add}\")\n",
    "    else:\n",
    "        print(\"No new tokens needed\")\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "def convert_epitope_format(sequence):\n",
    "    \"\"\"Convert [X] format to <epi>X</epi> format\"\"\"\n",
    "    return re.sub(r'\\[([A-Z])\\]', r'<epi>\\1</epi>', sequence)\n",
    "\n",
    "def format_mistral_prompt(antigen_sequence):\n",
    "    \"\"\"Format the antigen sequence for Mistral model\"\"\"\n",
    "    formatted_antigen = convert_epitope_format(antigen_sequence)\n",
    "    prompt = f\"Antigen: <s>{formatted_antigen}</s>\\nAntibody:\"\n",
    "    return prompt\n",
    "\n",
    "def load_mistral_model(model_path):\n",
    "    \"\"\"Load Mistral model with proper tokenizer setup and vocabulary resizing\"\"\"\n",
    "    print(f\"Loading Mistral model: {model_path}...\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    try:\n",
    "        # Try to load PEFT config first\n",
    "        config = PeftConfig.from_pretrained(model_path)\n",
    "        is_peft_model = True\n",
    "        base_model_name = config.base_model_name_or_path\n",
    "        print(f\"Detected PEFT model with base: {base_model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Not a PEFT model or couldn't load config: {e}\")\n",
    "        is_peft_model = False\n",
    "        base_model_name = model_path\n",
    "    \n",
    "    if is_peft_model:\n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            try:\n",
    "                print(\"Loading tokenizer from adapter path...\")\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "                expected_vocab_size = len(tokenizer)\n",
    "                print(f\"Tokenizer loaded with vocab size: {expected_vocab_size}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Couldn't load tokenizer from adapter path: {e}\")\n",
    "                print(\"Loading tokenizer from base model...\")\n",
    "                tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "                expected_vocab_size = 32005\n",
    "                print(f\"Using expected vocab size: {expected_vocab_size}\")\n",
    "            \n",
    "            # Set pad token if needed\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Load base model\n",
    "            print(f\"Loading base model: {base_model_name}\")\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Resize embeddings if needed\n",
    "            current_vocab_size = base_model.config.vocab_size\n",
    "            print(f\"Base model vocab size: {current_vocab_size}\")\n",
    "            \n",
    "            if current_vocab_size != expected_vocab_size:\n",
    "                print(f\"Resizing embeddings from {current_vocab_size} to {expected_vocab_size}\")\n",
    "                base_model.resize_token_embeddings(expected_vocab_size)\n",
    "                print(\"Embeddings resized successfully\")\n",
    "            \n",
    "            # Load PEFT adapters\n",
    "            print(\"Loading PEFT adapters...\")\n",
    "            model = PeftModel.from_pretrained(\n",
    "                base_model, \n",
    "                model_path,\n",
    "                is_trainable=False\n",
    "            )\n",
    "            print(\"PEFT model loaded successfully\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"size mismatch\" in str(e):\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"VOCABULARY SIZE MISMATCH DETECTED\")\n",
    "                print(\"=\"*60)\n",
    "                print(\"The model has a vocabulary size mismatch.\")\n",
    "                print(\"This typically happens when the model was fine-tuned with additional tokens.\")\n",
    "                print(\"\\nAttempting alternative loading method...\")\n",
    "                \n",
    "                # Try manual fix\n",
    "                model, tokenizer = load_mistral_model_with_manual_resize(model_path)\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "    else:\n",
    "        # Load as regular model\n",
    "        print(\"Loading as regular model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "    \n",
    "    # Setup tokenizer with special tokens\n",
    "    tokenizer, model = setup_mistral_tokenizer(tokenizer, model)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"Model loaded on device: {device}\")\n",
    "    print(f\"Final vocab size: {len(tokenizer)}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_mistral_model_with_manual_resize(model_path, target_vocab_size=32005):\n",
    "    \"\"\"Alternative loading method with manual vocabulary resize\"\"\"\n",
    "    print(\"Using manual vocabulary resize method...\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # Load PEFT config\n",
    "    config = PeftConfig.from_pretrained(model_path)\n",
    "    base_model_name = config.base_model_name_or_path\n",
    "    \n",
    "    # Load tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    except:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load base model\n",
    "    print(f\"Loading base model: {base_model_name}\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Force resize to target vocabulary size\n",
    "    print(f\"Force resizing embeddings to {target_vocab_size}\")\n",
    "    base_model.resize_token_embeddings(target_vocab_size)\n",
    "    \n",
    "    # Now load PEFT adapters\n",
    "    print(\"Loading PEFT adapters with resized model...\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        model_path,\n",
    "        is_trainable=False\n",
    "    )\n",
    "    \n",
    "    # Setup tokenizer\n",
    "    tokenizer, model = setup_mistral_tokenizer(tokenizer, model)\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully with manual resize!\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_mistral_antibody(model, tokenizer, antigen_seq):\n",
    "    \"\"\"Generate antibody sequence using Mistral model\"\"\"\n",
    "    try:\n",
    "        # Format prompt for Mistral\n",
    "        prompt = format_mistral_prompt(antigen_seq)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        \n",
    "        # Move to GPU\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Store the input length for later use\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=800,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "                use_cache=False,\n",
    "            )\n",
    "        \n",
    "        # Only decode the generated part (exclude the prompt)\n",
    "        generated_tokens = outputs[0][input_length:]\n",
    "        antibody_sequence = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        return antibody_sequence\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating sequence: {e}\")\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "def generate_antibodies_for_single_antigen(antigen_sequence, num_samples=20, model_path=\"silicobio/peleke-mistral-7b-instruct-v0.2\"):\n",
    "    \"\"\"Generate multiple antibody samples for a single antigen\"\"\"\n",
    "    \n",
    "    print(f\"Loading model: {model_path}\")\n",
    "    model, tokenizer = load_mistral_model(model_path)\n",
    "    \n",
    "    print(f\"\\nGenerating {num_samples} antibody samples for antigen...\")\n",
    "    print(f\"Antigen sequence: {antigen_sequence[:100]}...\")\n",
    "    print(f\"Converted format: {convert_epitope_format(antigen_sequence)[:100]}...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        for i in range(num_samples):\n",
    "            print(f\"Generating sample {i+1}/{num_samples}...\")\n",
    "            \n",
    "            # Generate antibody sequence\n",
    "            antibody_seq = generate_mistral_antibody(model, tokenizer, antigen_sequence)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'sample_id': f\"antibody_{i+1:02d}\",\n",
    "                'antigen_sequence': antigen_sequence,\n",
    "                'antibody_sequence': antibody_seq,\n",
    "                'sequence_length': len(antibody_seq),\n",
    "                'contains_error': 'ERROR' in antibody_seq\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Show progress\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  Generated {i + 1}/{num_samples} samples\")\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        print(\"Cleaning up model...\")\n",
    "        del model, tokenizer\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results_to_csv(results, filename=\"Mistral_antibodies.csv\"):\n",
    "    \"\"\"Save results to CSV file\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {filename}\")\n",
    "    print(f\"Total sequences generated: {len(df)}\")\n",
    "    print(f\"Successful generations: {len(df[~df['contains_error']])}\")\n",
    "    print(f\"Failed generations: {len(df[df['contains_error']])}\")\n",
    "    print(f\"Average sequence length: {df[~df['contains_error']]['sequence_length'].mean():.1f} characters\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_antibody_sequences(results):\n",
    "    \"\"\"Basic validation of generated antibody sequences\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANTIBODY SEQUENCE VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    valid_amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    validation_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        antibody_seq = result['antibody_sequence']\n",
    "        \n",
    "        # Skip error sequences\n",
    "        if result['contains_error']:\n",
    "            validation_results.append({\n",
    "                'sample_id': result['sample_id'],\n",
    "                'is_valid': False,\n",
    "                'error_type': 'Generation error',\n",
    "                'sequence_length': 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Check for valid amino acids only\n",
    "        seq_chars = set(antibody_seq.upper())\n",
    "        invalid_chars = seq_chars - valid_amino_acids\n",
    "        \n",
    "        # Check length (typical antibodies are 100-150 amino acids for Fv region)\n",
    "        seq_length = len(antibody_seq)\n",
    "        reasonable_length = 50 <= seq_length <= 300\n",
    "        \n",
    "        is_valid = len(invalid_chars) == 0 and reasonable_length\n",
    "        \n",
    "        validation_results.append({\n",
    "            'sample_id': result['sample_id'],\n",
    "            'is_valid': is_valid,\n",
    "            'sequence_length': seq_length,\n",
    "            'invalid_chars': list(invalid_chars) if invalid_chars else None,\n",
    "            'reasonable_length': reasonable_length\n",
    "        })\n",
    "    \n",
    "    # Summary\n",
    "    valid_count = sum(1 for v in validation_results if v['is_valid'])\n",
    "    total_count = len(validation_results)\n",
    "    \n",
    "    print(f\"Valid antibody sequences: {valid_count}/{total_count} ({100*valid_count/total_count:.1f}%)\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your single antigen sequence\n",
    "    antigen_sequence = \"ICLQKTSNQILKPKLISYTLGQSGTCITDPLLAMDEGYFAYSHLERIG[S][C][S][R]GVSKQRIIGVGEVLDRGDEVPSLFMTNVWTPPNPNTVYHCSAVYNNEFYYVLCAVSTVGDPI[L]NSTYWSGSLMMTRLAVKPKSNGGGYNQHQLALRSIEKGRYDKVMPYGPSGIKQGDTLYFPAVGFLVRTEFKYNDSNCPITKC[Q][Y]SKPENCRLSMG[I][R]PNSHYILRSGLLKYNLSDGENPKVVFIEISDQRLSIGSPSKIYDSLGQPVFYQAS[F]SWDTMIKFGDVLTVNPLVVNWRNNTVISR[P][G][Q][S][Q]CPRFNTCP[E]IC[W][E][G][V]YNDAFLIDRINWISAGVFLDSN[Q][T][A][E]NPVFTVFKDNEILYRAQLASE[D]T[N][A][Q]KTITNCFLLKNKIWCISLV[E][I][Y]D[T]GDNV[I]RPKLFAVKIPEQCTH\"\n",
    "    \n",
    "    print(\"Starting single antigen antibody generation...\")\n",
    "    \n",
    "    # Generate antibodies\n",
    "    results = generate_antibodies_for_single_antigen(\n",
    "        antigen_sequence=antigen_sequence,\n",
    "        num_samples=20\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    df = save_results_to_csv(results)\n",
    "    \n",
    "    # Validate sequences\n",
    "    validation_results = validate_antibody_sequences(results)\n",
    "    \n",
    "    # Show sample results\n",
    "    print(f\"\\nSample generated antibodies:\")\n",
    "    successful_results = [r for r in results if not r['contains_error']]\n",
    "    \n",
    "    for i, result in enumerate(successful_results[:3]):\n",
    "        print(f\"\\nSample {result['sample_id']}:\")\n",
    "        print(f\"Length: {result['sequence_length']} amino acids\")\n",
    "        print(f\"Sequence: {result['antibody_sequence'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\nGeneration completed! Check 'single_antigen_antibodies.csv' for full results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
